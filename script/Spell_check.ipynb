{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from wordsegment import load, segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = [line.rstrip('\\n') for line in open('../data/train_pos.txt')]\n",
    "pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Mismatch between array dtype ('<U217') and format specifier ('%.18e')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments)\u001b[0m\n\u001b[0;32m   1253\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m                     \u001b[0mfh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not numpy.str_",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-c1e256e6d674>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test.out'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments)\u001b[0m\n\u001b[0;32m   1256\u001b[0m                     raise TypeError(\"Mismatch between array dtype ('%s') and \"\n\u001b[0;32m   1257\u001b[0m                                     \u001b[1;34m\"format specifier ('%s')\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1258\u001b[1;33m                                     % (str(X.dtype), format))\n\u001b[0m\u001b[0;32m   1259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfooter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m             \u001b[0mfooter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfooter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcomments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Mismatch between array dtype ('<U217') and format specifier ('%.18e')"
     ]
    }
   ],
   "source": [
    "np.savetxt('test.out', x, delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "from wordsegment import load, segment\n",
    "\n",
    "def check_dict(word, dictionary):\n",
    "    if word in dictionary:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def correct_char_repetition(word):\n",
    "    word = word.lower()\n",
    "    occurance = [(k, sum(1 for i in g)) for k,g in groupby(word)]\n",
    "    if len(occurance)==1: \n",
    "        return word\n",
    "    if max([j for (_,j) in occurance]) > 2:\n",
    "        corrected_word = ''\n",
    "        for (i,j) in occurance:\n",
    "            if j>2:\n",
    "                corrected_word += 2*i\n",
    "            else:\n",
    "                corrected_word += i*j\n",
    "        return corrected_word\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "dictionary = Counter(words(open('../data/english_words.txt').read()))\n",
    "\n",
    "i = 1\n",
    "for key in list(dictionary.keys())[::-1]:\n",
    "    dictionary[key] = i\n",
    "    i += 1\n",
    "\n",
    "def P(word, N=sum(dictionary.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return dictionary[word] / N\n",
    "\n",
    "def separate(words):\n",
    "    return ' '.join(word for word in segment(words))\n",
    "\n",
    "def est_check(word):\n",
    "    return len(word)>4 and word[-3:]=='est' and word[:-3] in dictionary\n",
    "\n",
    "def correction(word): \n",
    "    #load()\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    \n",
    "    #delete repeated letters\n",
    "    word = correct_char_repetition(word)\n",
    "\n",
    "    #if degits return it\n",
    "    if word.isdigit():\n",
    "        return word\n",
    "\n",
    "    if word in dictionary:\n",
    "        return word\n",
    "    else:\n",
    "        if est_check(word):\n",
    "            word = word[:-2]\n",
    "        cand_word = max(candidates(word), key=P)\n",
    "        if cand_word in dictionary:\n",
    "            return cand_word\n",
    "        else:\n",
    "            return separate(word)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    print(known(edits1(word)))\n",
    "    print(known(edits2(word)))\n",
    "    return (known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in dictionary)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    #print(set(deletes + transposes + replaces + inserts))\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "{'cant'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cant'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'caannt'\n",
    "load()\n",
    "a = correction(word)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Lemma.name of Lemma('stack.n.01.stack')>   2\n",
      "<bound method Lemma.name of Lemma('batch.n.02.batch')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.deal')>   1\n",
      "<bound method Lemma.name of Lemma('batch.n.02.flock')>   1\n",
      "<bound method Lemma.name of Lemma('batch.n.02.good_deal')>   13\n",
      "<bound method Lemma.name of Lemma('batch.n.02.great_deal')>   10\n",
      "<bound method Lemma.name of Lemma('batch.n.02.hatful')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.heap')>   2\n",
      "<bound method Lemma.name of Lemma('batch.n.02.lot')>   13\n",
      "<bound method Lemma.name of Lemma('batch.n.02.mass')>   14\n",
      "<bound method Lemma.name of Lemma('batch.n.02.mess')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.mickle')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.mint')>   1\n",
      "<bound method Lemma.name of Lemma('batch.n.02.mountain')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.muckle')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.passel')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.peck')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.pile')>   3\n",
      "<bound method Lemma.name of Lemma('batch.n.02.plenty')>   2\n",
      "<bound method Lemma.name of Lemma('batch.n.02.pot')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.quite_a_little')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.raft')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.sight')>   1\n",
      "<bound method Lemma.name of Lemma('batch.n.02.slew')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.spate')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.stack')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.tidy_sum')>   0\n",
      "<bound method Lemma.name of Lemma('batch.n.02.wad')>   0\n",
      "<bound method Lemma.name of Lemma('push-down_list.n.01.push-down_list')>   0\n",
      "<bound method Lemma.name of Lemma('push-down_list.n.01.push-down_stack')>   0\n",
      "<bound method Lemma.name of Lemma('push-down_list.n.01.stack')>   0\n",
      "<bound method Lemma.name of Lemma('smokestack.n.01.smokestack')>   0\n",
      "<bound method Lemma.name of Lemma('smokestack.n.01.stack')>   0\n",
      "<bound method Lemma.name of Lemma('push-down_storage.n.01.push-down_storage')>   0\n",
      "<bound method Lemma.name of Lemma('push-down_storage.n.01.push-down_store')>   0\n",
      "<bound method Lemma.name of Lemma('push-down_storage.n.01.stack')>   0\n",
      "<bound method Lemma.name of Lemma('stack.v.01.stack')>   2\n",
      "<bound method Lemma.name of Lemma('stack.v.02.stack')>   0\n",
      "<bound method Lemma.name of Lemma('stack.v.02.pile')>   8\n",
      "<bound method Lemma.name of Lemma('stack.v.02.heap')>   1\n",
      "<bound method Lemma.name of Lemma('stack.v.03.stack')>   0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets('stack')\n",
    "for s in syns:\n",
    "    for l in s.lemmas():\n",
    "        print(l.name, ' ', str(l.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for w in wordnet.words():\n",
    "    words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thefile = open('wordnet_words.txt', 'w')\n",
    "for item in words:\n",
    "  thefile.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segments = separate(word)\n",
    "b = ' '.join(word for word in segments)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(dictionary.keys())[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pD.PyDictionary.getMeanings('wanhappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = 'fastest'\n",
    "word[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.metrics import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance(\"looved\",\"loved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
