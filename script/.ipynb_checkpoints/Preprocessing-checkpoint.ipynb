{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import spell_check\n",
    "from wordsegment import load, segment\n",
    "from collections import Counter\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = [line.rstrip('\\n') for line in open('../data/train_pos.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check(tweet_list):\n",
    "    '''Correct words'''\n",
    "    new_pos = []\n",
    "    for tweet in tweet_list:\n",
    "        words = [spell_check.correction(word) for word in tokenize(tweet)]\n",
    "        new_pos.append(' '.join(word for word in words))\n",
    "    return new_pos\n",
    "\n",
    "def remove_punctuation(tweet_list):\n",
    "    '''Remove all punctuations from string'''\n",
    "    new_pos = []\n",
    "    for tweet in tweet_list:\n",
    "        new_pos.append(\"\".join(l for l in tweet if l not in string.punctuation))\n",
    "    return new_pos\n",
    "\n",
    "def tokenize(text):\n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "def remove_stopwords(tweet_list):\n",
    "    '''Remove all stopwords'''\n",
    "    stop_words = [line.rstrip('\\n').lower() for line in open('../data/stopwords.txt')]\n",
    "    \n",
    "    new_pos = []\n",
    "    for tweet in tweet_list:\n",
    "        words = [word for word in tokenize(tweet) if not word in stop_words]\n",
    "        new_pos.append(' '.join(word for word in words))\n",
    "    return new_pos\n",
    "\n",
    "# delete whitespace\n",
    "def replace_smiley_slang(tweet_list):\n",
    "    #Smiley dataset\n",
    "    smiley_list = [line.rstrip('\\n').lower() for line in open('../data/smiley.txt')]\n",
    "    keys = np.asarray(smiley_list)[2*np.arange(int(len(smiley_list)/2))]\n",
    "    keys = [k.strip() for k in keys]\n",
    "    keys2 = [k.replace('-','') for k in keys]\n",
    "    values = list(np.asarray(smiley_list)[2*np.arange(int(len(smiley_list)/2))+1])\n",
    "    smiley_dict = dict(zip(keys+keys2, values+values))\n",
    "    \n",
    "    #Slangs datasets 1\n",
    "    slang_list = [line.rstrip('\\n') for line in open('../data/slang2.txt')]\n",
    "    keys = np.asarray(slang_list)[:int(len(slang_list)/2)]\n",
    "    keys = [k.strip() for k in keys]\n",
    "    values = np.asarray(slang_list)[int(len(slang_list)/2):]\n",
    "    slang_dict = dict(zip(keys, values))\n",
    "    slang_dict\n",
    "    \n",
    "    #Slang dataset 2\n",
    "    slang_list = [line.rstrip('\\n').lower() for line in open('../data/slang.txt')]\n",
    "    keys = np.asarray(slang_list)[2*np.arange(int(len(slang_list)/2))]\n",
    "    keys = [k.strip() for k in keys]\n",
    "    values = list(np.asarray(slang_list)[2*np.arange(int(len(slang_list)/2))+1])\n",
    "    slang_dict2 = dict(zip(keys, values))\n",
    "    \n",
    "    new_pos = []\n",
    "    for tweet in tweet_list:\n",
    "        words = tokenize(tweet)\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in smiley_dict.keys():\n",
    "                words[i] = smiley_dict[words[i]]\n",
    "            elif words[i] in slang_dict.keys():\n",
    "                words[i] = slang_dict[words[i]]\n",
    "            elif words[i] in slang_dict2.keys():\n",
    "                words[i] = slang_dict2[words[i]]\n",
    "        new_pos.append(' '.join(word for word in words))\n",
    "    return new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<user> i dm'd you what to do get your followers to follow you on my your new twitter . where is this shindig ? x\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace smileys and slogans\n",
    "pos = replace_smiley_slang(pos)\n",
    "pos[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user i dmd you what to do get your followers to follow you on my your new twitter  where is this shindig  x'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctiuations\n",
    "pos = remove_punctuation(pos)\n",
    "pos[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user i dmd followers follow twitter shindig x'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopWords\n",
    "pos = remove_stopwords(pos)\n",
    "pos[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spell check\n",
    "pos_f = []\n",
    "for i in range(20,40):\n",
    "    pos_f.append([spell_check.correction(s) for s in pos[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_f = []\n",
    "for tweet in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'usereee'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load()\n",
    "spell_check.correction('iamhappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove punctiuation\n",
    "pos = [tokenize(remove_punctuation(p)) for p in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat_pos = [word for tweet in pos for word in tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'fml' in flat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter['yw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = Counter(flat_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['user',  i', 'you', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pos_f:\n",
    "    print('##', ' '.join(s for s in p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pos[10:20]:\n",
    "    print('##', ' '.join(s for s in p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'201a5'.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_check.correction('abcd105')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_list = [line.rstrip('\\n').lower() for line in open('../data/english_words.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_check(s):\n",
    "    return s in words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = []\n",
    "for tweet in pos[:10]:\n",
    "    print(tweet)\n",
    "    check.append([w for w in tweet if not dict_check(w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
