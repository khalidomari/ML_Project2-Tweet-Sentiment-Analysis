{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_hash_tokenize(tweet):\n",
    "    new = tweet.replace('#', '')\n",
    "    return new.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '../data/'\n",
    "pos = np.asarray([line.rstrip('\\n').lower() for line in open(BASE+'train_pos_full.txt', encoding='utf8')])\n",
    "neg = np.asarray([line.rstrip('\\n').lower() for line in open(BASE+'train_neg_full.txt', encoding='utf8')])\n",
    "test = np.asarray([line.rstrip('\\n').lower() for line in open(BASE+'test_data.txt', encoding='utf8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tkn = [remove_hash_tokenize(tweet) for tweet in pos] \n",
    "neg_tkn = [remove_hash_tokenize(tweet) for tweet in neg] \n",
    "test_tkn = [remove_hash_tokenize(tweet) for tweet in test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tkn = [w for tokens in pos_tkn+neg_tkn+test_tkn for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39499657"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571468"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkn_counter = Counter(all_tkn)\n",
    "len(tkn_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Diictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upload_dict(file, BASE = './dict/', hltud=False):\n",
    "    path = BASE + file\n",
    "    words = np.asarray([line.rstrip('\\n').lower() for line in open(path)])\n",
    "    if hltud:\n",
    "        keys = [w.split()[1] for w in words]\n",
    "        values = [w.split()[3] for w in words]\n",
    "    else:\n",
    "        keys = [w.split()[0] for w in words]\n",
    "        values = [w.split()[1] for w in words]\n",
    "    return dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnlp = upload_dict('emnlp_dict.txt')\n",
    "luulu = upload_dict('luulu_typo-corpus-r1.txt')\n",
    "hltd = upload_dict('hltutdallas.txt', hltud=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_dict = set(list(luulu)+list(emnlp)+list(hltd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49349"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spell_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload english dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '../data/dictionaries/'\n",
    "english_words = np.asarray([line.rstrip('\\n').lower() for line in open(BASE+'english_words.txt')])\n",
    "idx = np.arange(int(len(english_words)/3))\n",
    "english_dictionary = dict(zip(english_words[3*idx+1], english_words[3*idx+1]))\n",
    "freq =  dict(zip(english_words[3*idx+1], english_words[3*idx+2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36662, 36662)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_dictionary), len(set(english_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload acronyms / smileys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(sentence, contrac_dict={}):\n",
    "\t'replace contractions in sentence and remove punctuation'\n",
    "\ttokens = sentence.split()\n",
    "\tnew_tokens = []\n",
    "\tfor token in tokens:\n",
    "\t\tif token in contrac_dict:\n",
    "\t\t\tnew_tokens.append(contrac_dict[token])\n",
    "\t\tif len(token)>1:\n",
    "\t\t\tnew_tokens.append(''.join(c for c in token if c not in string.punctuation))\n",
    "\treturn ' '.join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Acronyms\n",
    "acronyms = np.asarray([line.rstrip('\\n').lower() for line in open(BASE+'netlingo_acronyms.txt')])\n",
    "idx = np.arange(int(len(acronyms)/2))\n",
    "acronyms_dict = dict(zip(acronyms[2*idx], acronyms[2*idx+1]))\n",
    "#Remove multi explications\n",
    "for key in acronyms_dict:\n",
    "\tacronyms_dict[key] = acronyms_dict[key].split('/ ')[0]\n",
    "  #correct descriptions\n",
    "for key in acronyms_dict:\n",
    "\tacronyms_dict[key] = correct(acronyms_dict[key])\n",
    "\n",
    "## Smileys\n",
    "smileys = np.asarray([line.rstrip('\\n').lower() for line in open(BASE+'netlingo_smileys.txt')])\n",
    "idx = np.arange(int(len(smileys)/2))\n",
    "smileys_dict = dict(zip(smileys[2*idx], smileys[2*idx+1]))\n",
    "#Remove multi explications\n",
    "for key in smileys_dict:\n",
    "\tsmileys_dict[key] = smileys_dict[key].split('- ')[0]\n",
    "#remove '-' from smiley and add it\n",
    "keys = list(smileys_dict.keys())\n",
    "for s in keys:\n",
    "    if '-' in s and len(s)>2:\n",
    "        smileys_dict[s.replace('-', '')] = smileys_dict[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5032, 412)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acronyms), len(smileys_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_presence(tkn_counter, word_dict):\n",
    "    count = 0\n",
    "    for w in tkn_counter:\n",
    "        if w in word_dict:\n",
    "            count += tkn_counter[w]\n",
    "    print('{}% found'.format(round(count*100/len(all_tkn), 2)))\n",
    "\n",
    "def get_unknown_words(tkn_counter, word_dict, Threshold=100):\n",
    "    unknown_words = []\n",
    "    for w in tkn_counter:\n",
    "        if w not in word_dict:\n",
    "            unknown_words.append(w)\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.53% found\n"
     ]
    }
   ],
   "source": [
    "count_presence(tkn_counter, spell_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.16% found\n"
     ]
    }
   ],
   "source": [
    "count_presence(tkn_counter, english_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = list(english_dictionary.keys())+list(spell_dict)+list(smileys_dict.keys())+list(acronyms_dict.keys())\n",
    "tmp = set(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.73% found\n"
     ]
    }
   ],
   "source": [
    "count_presence(tkn_counter, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513624"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown = get_unknown_words(tkn_counter, tmp)\n",
    "len(unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.043757012877352"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([tkn_counter[w] for w in unknown if len(w)==1])*100/len(all_tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn_counter['Ã—']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
